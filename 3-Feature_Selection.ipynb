{"cells":[{"cell_type":"markdown","metadata":{"id":"GXiu4l_dmwav"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiWWe3OQm1qI","outputId":"fa56e38c-9fc6-4c6b-c938-759ecbb9396a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: liac-arff in /opt/anaconda3/lib/python3.9/site-packages (2.5.0)\n"]}],"source":["!pip install liac-arff"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# !pip install -U scikit-learn scipy matplotlib"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# !pip install requests"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4c__rTtNmx6o"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import arff\n","import requests\n","\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import cross_val_score"]},{"cell_type":"markdown","metadata":{"id":"nfWLkVaEmwYC"},"source":["## Import Data"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"eo5YvKzpoQIU"},"outputs":[],"source":["# get data from online\n","training_arff = requests.get('https://raw.githubusercontent.com/juwon0502/MIS-373-Predictive-Analytics/master/datasets/bank-training.arff')\n","testing_arff = requests.get('https://raw.githubusercontent.com/juwon0502/MIS-373-Predictive-Analytics/master/datasets/bank-NewCustomers.arff')\n","\n","# read as arff file\n","training_arff = arff.load(training_arff.text)\n","testing_arff = arff.load(testing_arff.text)\n","col_val = [attribute[0] for attribute in training_arff['attributes']]\n","\n","# transform arff file into pandas dataframe\n","training_df = pd.DataFrame(training_arff['data'], columns = col_val)\n","testing_df = pd.DataFrame(testing_arff['data'], columns = col_val)\n","meta = training_arff['attributes']\n","\n","def clean_df(df):\n","  cols = list(df.columns)\n","  for col in cols:\n","    try:\n","      df = df.replace({col: {'YES': True, 'NO': False}})\n","    except:\n","      pass\n","    pass\n","  return df\n","\n","training_df = clean_df(training_df)\n","training_df_dummy = pd.get_dummies(training_df)\n","testing_df_dummy = pd.get_dummies(clean_df(testing_df))"]},{"cell_type":"markdown","metadata":{"id":"A7djIBYlBTsS"},"source":["## Sample Model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"iSIUifR1BWIT"},"outputs":[],"source":["X = training_df_dummy.drop(columns = ['pep'])\n","y = training_df_dummy.pep\n","model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5).fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"IdRK0PTmdQIL"},"source":["Filter method"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"N6aTAkuRMPPF","outputId":"9ff9e55e-33a5-4058-d56c-630e317ef39e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature 0: 0.015596\n","Feature 1: 0.001894\n","Feature 2: 0.000000\n","Feature 3: 0.067190\n","Feature 4: 0.002569\n","Feature 5: 0.037338\n","Feature 6: 0.022639\n","Feature 7: 0.004780\n","Feature 8: 0.000000\n","Feature 9: 0.008924\n","Feature 10: 0.048065\n","Feature 11: 0.034976\n","Feature 12: 0.009533\n","Feature 13: 0.016434\n","[('children', 0.0671895933178468), ('region_INNER_CITY', 0.04806519023889355), ('save_act', 0.037337882143938295), ('region_RURAL', 0.03497593955579603), ('current_act', 0.02263898929701913), ('region_TOWN', 0.016433912239000925), ('age', 0.015595932816462543), ('region_SUBURBAN', 0.009532940081643293), ('sex_MALE', 0.008923852410055666), ('mortgage', 0.004779755765959015), ('car', 0.0025691370001930114), ('income', 0.0018936945902052749), ('married', 0.0), ('sex_FEMALE', 0.0)]\n","CV with all attributes: 0.8433333333333334\n","*********************\n","CV without children is 0.6166666666666667\n","CV without region_INNER_CITY is 0.8433333333333334\n","CV without save_act is 0.8333333333333334\n","CV without region_RURAL is 0.8416666666666668\n","CV without current_act is 0.8433333333333334\n","CV without region_TOWN is 0.8416666666666668\n","CV without age is 0.8416666666666666\n","CV without region_SUBURBAN is 0.8433333333333334\n","CV without sex_MALE is 0.8433333333333334\n","CV without mortgage is 0.8266666666666665\n","CV without car is 0.8416666666666668\n","CV without income is 0.7966666666666666\n","CV without married is 0.8283333333333334\n","CV without sex_FEMALE is 0.8433333333333334\n"]}],"source":["#Libraries for feature ranking based on filter methods\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import mutual_info_classif\n","X = training_df_dummy.drop(columns = ['pep'])\n","y = training_df_dummy.pep\n","\n","# Use mutual information to rank the features\n","selector = SelectKBest(mutual_info_classif)\n","selector.fit(X, y)\n","scores = selector.scores_\n","\n","# Print the scores for each feature\n","for i, score in enumerate(selector.scores_):\n","    print(\"Feature %d: %f\" % (i, score))\n","\n","# Get the indices of the selected features\n","selected_features_indices = selector.get_support(indices=True)\n","\n","# Get the names of all features\n","# feature_names = X.feature_names\n","feature_names=list(X.columns)\n","\n","# Create a dictionary that maps feature names to their scores\n","score_dict = dict(zip(feature_names, scores))\n","\n","\n","#To Test: dropping only one and put it back\n","#Sort the dictionary by scores in descending order\n","sorted_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n","print (sorted_dict)\n","model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5,random_state=0).fit(X, y)\n","mycv = cross_val_score(model, X, y, cv = 10)\n","print('CV with all attributes:', mycv.mean())\n","\n","print('*********************')\n","for x in sorted_dict:\n","    #print (x[0],x[1])\n","    #Drop the feature we are analyzing\n","    X1 = X.drop(columns = [x[0]])\n","\n","    #Fit the model without the feature\n","    model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5,random_state=0).fit(X1, y)\n","    mycv = cross_val_score(model, X1, y, cv = 10)\n","    #We can see dropping which features gives higher accuracy than when have all features, 0.843\n","    print(f'CV without {x[0]} is {mycv.mean()}')\n","\n","\n","# #To Test: dropping from the least important to the most important\n","# #Sort the dictionary by scores in ascending order\n","# sorted_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=False)\n","# print (sorted_dict)\n","# model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5,random_state=0).fit(X, y)\n","# mycv = cross_val_score(model, X, y, cv = 10)\n","# print('CV with all attributes:', mycv.mean())\n","# print('*********************')\n","# for x in sorted_dict:\n","#     #print (x[0],x[1])\n","#     #Drop the current least important feature\n","#     #Ensure we have at least one feature left\n","#     if len(X.columns) > 1:\n","#         X = X.drop(columns = [x[0]])\n","\n","#     #Fit the model without the feature\n","#     model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5,random_state=0).fit(X, y)\n","#     mycv = cross_val_score(model, X, y, cv = 10)\n","#     #We can see when CV without region_TOWN, the accuracy rate is the highest\n","#     print(f'CV without {x[0]} is {mycv.mean()}')    "]},{"cell_type":"markdown","metadata":{"id":"Hz2xVuRj5jG6"},"source":["Wrapper"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGZ3tZFZkpvp","outputId":"9d86307c-30ec-45a7-da76-22708f238f68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature 0: Rank 5\n","Feature 1: Rank 1\n","Feature 2: Rank 3\n","Feature 3: Rank 1\n","Feature 4: Rank 13\n","Feature 5: Rank 4\n","Feature 6: Rank 12\n","Feature 7: Rank 2\n","Feature 8: Rank 11\n","Feature 9: Rank 10\n","Feature 10: Rank 6\n","Feature 11: Rank 9\n","Feature 12: Rank 8\n","Feature 13: Rank 7\n"]}],"source":["#RFE is for feature ranking here, one of the wrapper methods\n","from sklearn.feature_selection import RFE\n","\n","X = training_df_dummy.drop(columns = ['pep'])\n","y = training_df_dummy.pep\n","model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5).fit(X, y)\n","\n","selector = RFE(model, n_features_to_select=2, step=1)\n","selector.fit(X, y)\n","\n","# Print the ranking of each feature\n","for i, rank in enumerate(selector.ranking_):\n","    print(\"Feature %d: Rank %d\" % (i, rank))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KolmgxcomWoT","outputId":"242397d9-0c7b-4813-83fc-e526cb502e41"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC with all features:  0.8589720776805274\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_sequential.py:211: FutureWarning: Leaving `n_features_to_select` to None is deprecated in 1.0 and will become 'auto' in 1.3. To keep the same behaviour as with None (i.e. select half of the features) and avoid this warning, you should manually set `n_features_to_select='auto'` and set tol=None when creating an instance.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["AUC with only the selected features:  0.8750020918906373\n","Prediction from CV, AUC with all features:  0.8561864672428463\n","Prediction from CV, AUC with selected features:  0.8726825937038198\n","Selected features:  ['income', 'married', 'children', 'car', 'save_act', 'mortgage', 'region_INNER_CITY']\n"]}],"source":["# Import the libraries for feature selection based on wrapper methods\n","# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n","from sklearn.feature_selection import SequentialFeatureSelector as SFS\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import roc_auc_score\n","\n","X = training_df_dummy.drop(columns = ['pep'])\n","y = training_df_dummy.pep\n","folds=3\n","\n","\n","#Classification tree with all features\n","model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5,random_state=0).fit(X, y)\n","#Print the mean AUC score with all features\n","print('AUC with all features: ', cross_val_score(model, X, y, cv=folds, scoring='roc_auc').mean())\n","#Store mean AUC score based on predicitons made from cross validation\n","#Need [:, 1] to select the probability of being positive\n","a=roc_auc_score(y, cross_val_predict(model, X, y,method='predict_proba', cv=folds)[:, 1])\n","#The predicitons made from cross validation\n","b= cross_val_predict(model, X, y,method='predict_proba', cv=folds)\n","\n","\n","#Backward feature selection model, need the tree model first\n","sfs = SFS(model,direction='backward',scoring='roc_auc', cv=folds)\n","#Get the names of all features\n","feature_names=list(X.columns)\n","\n","#Feature selection based on our data\n","sfs1 = sfs.fit(X, y)\n","#Change X to include only the selected features in X1\n","X1=sfs1.transform(X)\n","#Store the selected features' names\n","fn=[ feature_names[i] for i, f in enumerate(sfs1.get_support()) if f ]\n","\n","#Classification tree with only the selected features\n","model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5,random_state=0).fit(X1, y)\n","#Print the mean AUC score with only the selected features\n","print('AUC with only the selected features: ', cross_val_score(model, X1, y, cv=folds,scoring='roc_auc').mean())\n","#Store mean AUC score based on predicitons made from cross validation\n","a1=roc_auc_score(y, cross_val_predict(model, X1, y,method='predict_proba', cv=folds)[:, 1])\n","#The predicitons made from cross validation\n","b1= cross_val_predict(model, X1, y,method='predict_proba', cv=folds)\n","\n","\n","print('Prediction from CV, AUC with all features: ', a)\n","print('Prediction from CV, AUC with selected features: ', a1)\n","print('Selected features: ', fn)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0c72BEe9Qwc0","outputId":"dbde79a9-20fe-42a2-e556-cd9e373f192b"},"outputs":[{"data":{"text/plain":["array([[0.12121212, 0.87878788],\n","       [0.875     , 0.125     ],\n","       [0.89473684, 0.10526316],\n","       ...,\n","       [0.89473684, 0.10526316],\n","       [0.45      , 0.55      ],\n","       [0.57142857, 0.42857143]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["b"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.12121212, 0.87878788],\n","       [0.875     , 0.125     ],\n","       [0.83333333, 0.16666667],\n","       ...,\n","       [0.89473684, 0.10526316],\n","       [0.45      , 0.55      ],\n","       [0.57142857, 0.42857143]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["b1"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
